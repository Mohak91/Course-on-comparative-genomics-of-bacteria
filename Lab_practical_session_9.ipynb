{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNynLCqg8ntqIoqxpkxf8dn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohak91/Course-on-comparative-genomics-of-bacteria/blob/main/Lab_practical_session_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Entropy\n",
        "\n",
        "What is entropy?\n",
        "- Measure used to quantify disorderedness.\n",
        "- Higher entropy signifies higher level of disorderedness.\n",
        "- A DNA subsequence comprising of a single nucleotide, for example, AAAAAAAAAAAAAAAAAAAAA is highly ordered and low complexity in terms of the kinds of nucleotides that make up this subsequence ( here just one kind i.e Adenine). Therefore, the entropy would be low.\n",
        "- A subsequence \"AAAAAAAATTTTTTTT\" will have a higher entropy, since it is relatively more complex than the sequence above i.e the homopolymer of A, in the previous point. However, the entropy of \"ATATATATATATATAT\" will be the same as \"AAAAAAAATTTTTTTT\". We will see why later.\n",
        "- Finally, a sequence with \"Ã„TGCATGCATGCATGC\" will have relatively highest entropy, since it is the most complex, as compared to the subsequences in the previous two points. It contains all kinds of nucleotides in same proportions.\n",
        "\n",
        "Entropy, in the context of genomes, help us get an idea of how local regions within a genome vary with respect to their base compositions. We can get an idea of what regions are repeat regions or have unsual base properties as compared to the rest of the genome. We will see this in detail, using practicals as well. \n",
        "\n",
        "####One question to think at this point is: What is so special about studying sequence evolution like repeat regions in a genome?\n",
        "\n",
        "Please note: This is just one context in which entropy as a measure is used in Bioinformatics i.e studying sequence/genome evolution and its peculiar properties.\n",
        "\n",
        "####Can you think of other scenarios where entropy can be used in Bioinformatics?"
      ],
      "metadata": {
        "id": "NyHbe0th4nSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Quantifying entropy\n",
        "\n",
        "Let us build an intuition that we discussed in class.\n",
        "\n",
        "Entropy is average surprise. \n",
        "\n",
        "What is surprise? Let's understand that.\n",
        "\n",
        "Think of a scenario where in a population, either a person likes watching movies. Let's mark that person as M. Another person who doesn't like watching movies. Let's mark that person as D.\n",
        "\n",
        "The population would look like below:\n",
        "\n",
        "MDMDMDMDMDMDMDMDMDMDMDMDMDMDMMDMDMDMDMDMDMDMDMD\n",
        "\n",
        "Let's segregate some of these people into three groups randomly.\n",
        "\n",
        "Let's say you end up getting the following three groups:\n",
        "- GROUP 1: MMMMD\n",
        "- GROUP 2: DDDDM\n",
        "- GROUP 3: DDMM\n",
        "\n",
        "If I close my eyes and choose a person from Group 1 and that person turns out to be M. Will that be surprising? Answer is NO. On the other hand, if I choose a person and that person is D, now that would be surprising. Right?\n",
        "\n",
        "Same way, If I close my eyes and choose a person from Group 2 and that person turns out to be D. Will that be surprising? Answer is NO. On the other hand, if I choose a person and that person is M, now that would be surprising. Right?\n",
        "\n",
        "Finally, we would be equally surprised (or not surprised) if I choose a person from Group 3 and it turns out to be M or D.\n",
        "\n",
        "####From above, what relationship can be deduced between surprise and probability?\n",
        "\n",
        "Surprise = 1 / Probability\n",
        "\n",
        "There is however one problem with this. It does not include one use case. \n",
        "\n",
        "Let us understand what.\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "To understand the problem with the current version of the equation above, let us flip the coin 10,00,000 times. Let us say it always turns up Heads. It is then safe to say that the coin is biased, where most likely both the sides are heads. Makes sense?\n",
        "\n",
        "Therefore,\n",
        "\n",
        "p(H) = 1, NOT SURPRISING AT ALL!!\n",
        "\n",
        "p(T) = 0\n",
        "\n",
        "But if we plug p(H) in the equation we wrote above, Surprise comes out to be 1. That does not makes sense. Surprise should be 0 if the p(H) = 1. Right?\n",
        "\n",
        "Let's change the equation to accomodate the above scenario!!!\n",
        "\n",
        "LOGARITHMS to the rescue:\n",
        "\n",
        "2^0 = 1\n",
        "\n",
        "2^1 = 2\n",
        "\n",
        "2^2 = 3 ...\n",
        "\n",
        "In terms of log, this can be written as\n",
        "\n",
        "log2(1) = 0 (read as log base 2 of 1 is 0)\n",
        "log2(2) = 1\n",
        "log2(3) = 2 ...\n",
        "\n",
        "Therefore, we can write\n",
        "\n",
        "Surprise = log2(1/p(H))\n",
        "\n",
        "If p(H) = 1, using the above formula, Surprise = 0\n",
        "\n",
        "That makes sense now!\n",
        "\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "Now let's say p(H) = 0.9 and p(T) = 0.1\n",
        "\n",
        "Surprise(H) = log2( 1 / 0.9) = log2(1) -log2(0.9) = 0 - (-0.15) = 0.15\n",
        "\n",
        "Surprise(T) = log2( 1 / 0.1) = log2(1) -log2(0.1) = 0 - (-3.32) = 3.32\n",
        "\n",
        "If we flip the same coin three times and we get HHT on three flips, what would be the total surprise in these three flips?\n",
        "\n",
        "Surprise(HHT) = log2(1/p(HHT))\n",
        "\n",
        "p(HHT) = 0.9 * 0.9 * 0.1, Right?\n",
        "\n",
        "Surprise(HHT) = log2(1/0.9 * 0.9 * 0.9) = log2(1) - log2(0.9 * 0.9 * 0.9 )\n",
        "= 0 - [log2(0.9) + log2(0.9) + log2(0.1)] = - [ (-0.15) + (-0.15) + (-3.32)] = 0.15 + 0.15 + 3.32 = Surprise(H) + Surprise(H) + Surprise(T) = 3.62\n",
        "\n",
        "Therefore, Surprise(HHT) = Surprise(H) + Surprise(H) + Surprise(T)\n",
        "\n",
        "What would be the total surprise if you flipped the coin three times and the sequence of H and T that you got was : TTH? Would it be more surprising to see this sequence when compared to HHT? And why? [Remember p(H) = 0.9 and p(T) = 0.1]\n",
        "\n",
        "Finally, let's flip this same coin 100 times. What would be the total surprise?\n",
        "\n",
        "Now in this question we do not know the chain of events or the sequence of events like we did previously. So to calculated the total surprise we have to find the expected number of heads and tails based on their probabilities and total number of coin flips.\n",
        "\n",
        "So for a coin flip that is carried out 100 times, with p(H) = 0.9 and p(T) = 0.1,\n",
        "\n",
        "The expected number of heads = 0.9 * 100 = 90\n",
        "The expected number of tails = 0.1 * 100 = 10\n",
        "\n",
        "if the surprise(H) = 0.15 and surprise(T) = 3.32, as calculated above,\n",
        "\n",
        "for 90 heads surprise(90H) = 90 * 0.15\n",
        "and, 10 tails surprise(10T) = 10 * 3.32\n",
        "\n",
        "Therefore the total surprise = Surprise(100 coin flips) = Surprise(90H) + Surprise(10T) = (90 * 0.15) + (10 * 3.32) = (0.9 * 100 * 0.15) + (0.1 * 100 * 3.32).\n",
        "\n",
        "Does this make sense?\n",
        "\n",
        "Now this is the total surprise for 100 coin flips. If one were to calculate the average surprise for any given coin flip for a coin that has been flipped 100 times with p(H)=0.9 and p(T) = 0.1, one can use the following formula:\n",
        "\n",
        "Average = Sum of observations / Number of observations\n",
        "\n",
        "Here we want to calculate Average surprise,\n",
        "\n",
        "Sum of surprises = total surprise = (0.9 * 100 * 0.15) + (0.1 * 100 * 3.32), that we calculated above.\n",
        "\n",
        "Number of observations = 100\n",
        "\n",
        "Therefore, average surprise = [(0.9 * 100 * 0.15) + (0.1 * 100 * 3.32)] / 100.\n",
        "\n",
        "This simplifies to:\n",
        "\n",
        "Average surprise = (0.9  * 0.15) + (0.1 * 3.32) = p(H) * S(H) + p(T) * S(T) = p(H) * log2(1/p(H)) + p(T) * log2(1/p(T))\n",
        "\n",
        "###Average surprise is called entropy. We just derived the formula for entropy.\n",
        "\n",
        "Entropy = Average surprise = [p(H) * log2(1/p(H))] + [p(T) * log2(1/p(T))]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L3djf8nM7mlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Entropy for genomic sequences\n",
        "\n",
        "Extending the above formula to genomic sequences,\n",
        "\n",
        "Entropy = Average surprise = [p(T) * log4(1/p(T))] + [p(G) * log4(1/p(G))] + [p(A) * log4(1/p(A))] + [p(C) * log4(1/p(C))]\n",
        "\n",
        "Note how the base has been changed to 4 from 2. Since the number of possible outcomes are now 4 (A,T,G,C) in a genomic sequence. In the coin example base was 2, since there were two outcomes (H or T). \n",
        "\n",
        "###What would you change the base to if you have to calculate the entropy of a protein sequence? \n",
        "\n",
        "Changing base to the number of outcomes ensures the value of entropy to be between 0 and 1. That is all!!\n",
        "\n",
        "Entropy = 0 would mean that there is high orderedness in the sequence i.e low complexity. Example: AAAAAAAAAAAA\n",
        "\n",
        "Entropy = 1 would mean that there is low orderedness in the sequence i.e high complexity. Example: ATCGATCGATCG\n",
        "\n",
        "\n",
        "###Calculate the entropy ( average surprise) of the following sequence:\n",
        "\n",
        "ATCGATCGATCATTATATAAATTATATATAAAAAAAAA\n",
        "\n"
      ],
      "metadata": {
        "id": "ULTcY5sMDoDT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQEvE32q4iBd"
      },
      "outputs": [],
      "source": []
    }
  ]
}